(vl-bert) c828w038@cmlgrad07:~/Visual-grounding/VLBERT/VL-BERT
$./scripts/dist_run_single.sh 1 refcoco/train_end2end.py cfgs/refcoco/base_gt_boxes_4x16G.yaml checkpoints/ 
hello  0 proc per node 1
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
/fs/classhomes/fall2020/cmsc828w/c828w038/Visual-grounding/VLBERT/VL-BERT/refcoco/../refcoco/function/config.py:178: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  exp_config = edict(yaml.load(f))
Namespace(cfg='cfgs/refcoco/base_gt_boxes_4x16G.yaml', cudnn_off=False, dist=True, do_test=False, log_dir='checkpoints/./output/vl-bert/refcoco+/base_gt_boxes_4x16G/train_train/tensorboard_logs', model_dir='checkpoints/', partial_pretrain=None, slurm=False)
{'CHECKPOINT_FREQUENT': 1,
 'DATASET': {'ADD_IMAGE_AS_A_BOX': True,
             'ANSWER_VOCAB_FILE': '',
             'ANSWER_VOCAB_SIZE': 3129,
             'APPEND_INDEX': False,
             'BASIC_ALIGN': False,
             'CACHE_MODE': False,
             'DATASET': 'refcoco+',
             'DATASET_PATH': './data/coco',
             'IGNORE_DB_CACHE': True,
             'LABEL_INDEX_IN_BATCH': -1,
             'MASK_SIZE': 14,
             'ONLY_USE_RELEVANT_DETS': True,
             'PROPOSAL_SOURCE': 'official',
             'QA2R_AUG': False,
             'QA2R_NOQ': False,
             'ROOT_PATH': './',
             'TASK': 'Q2AR',
             'TEST_ANNOTATION_FILE': '',
             'TEST_BOXES': 'gt',
             'TEST_IMAGE_SET': 'test',
             'TRAIN_ANNOTATION_FILE': '',
             'TRAIN_BOXES': 'gt',
             'TRAIN_IMAGE_SET': 'train',
             'USE_IMDB': True,
             'VAL_ANNOTATION_FILE': '',
             'VAL_BOXES': 'gt',
             'VAL_IMAGE_SET': 'val',
             'ZIP_MODE': False},
 'GPUS': '0,1,2,3',
 'LOG_FREQUENT': 100,
 'MODEL_PREFIX': 'vl-bert_base_res101_refcoco',
 'MODULE': 'ResNetVLBERT',
 'NETWORK': {'ANS_LOSS_TYPE': 'bce',
             'ANS_LOSS_WEIGHT': 1.0,
             'BERT_ALIGN_ANSWER': True,
             'BERT_ALIGN_QUESTION': True,
             'BERT_FROZEN': False,
             'BERT_MODEL_NAME': './model/pretrained_model/bert-base-uncased',
             'BERT_PRETRAINED': '',
             'BERT_PRETRAINED_EPOCH': 0,
             'BERT_USE_LAYER': -2,
             'BERT_WITH_MLM_LOSS': False,
             'BERT_WITH_NSP_LOSS': False,
             'BLIND': False,
             'CLASSIFIER_DROPOUT': 0.0,
             'CLASSIFIER_HIDDEN_SIZE': 1024,
             'CLASSIFIER_PRETRAINED': False,
             'CLASSIFIER_SIGMOID': False,
             'CLASSIFIER_SIGMOID_LOSS_POSITIVE_WEIGHT': 1.0,
             'CLASSIFIER_TYPE': '2fc',
             'CNN_LOSS_WEIGHT': 1.0,
             'ENABLE_CNN_REG_LOSS': False,
             'IMAGE_C5_DILATED': True,
             'IMAGE_FEAT_PRECOMPUTED': False,
             'IMAGE_FINAL_DIM': 768,
             'IMAGE_FROZEN_BACKBONE_STAGES': [1, 2],
             'IMAGE_FROZEN_BN': True,
             'IMAGE_NUM_LAYERS': 101,
             'IMAGE_PRETRAINED': './model/pretrained_model/resnet101-pt-vgbua',
             'IMAGE_PRETRAINED_EPOCH': 0,
             'IMAGE_SEMANTIC': False,
             'IMAGE_STRIDE_IN_1x1': True,
             'NO_GROUNDING': False,
             'NO_OBJ_ATTENTION': False,
             'OUTPUT_CONV5': False,
             'PARTIAL_PRETRAIN': './model/pretrained_model/vl-bert-base-e2e.model',
             'PARTIAL_PRETRAIN_PREFIX_CHANGES': ['vlbert.mvrc_head.transform->final_mlp.0',
                                                 'module.vlbert.mvrc_head.transform->module.final_mlp.0',
                                                 'vlbert->vlbert',
                                                 'module.vlbert->module.vlbert'],
             'PIXEL_MEANS': [102.9801, 115.9465, 122.7717],
             'PIXEL_STDS': [1.0, 1.0, 1.0],
             'REPLACE_OBJECT_CHANGE_LABEL': True,
             'VLBERT': {'attention_probs_dropout_prob': 0.1,
                        'hidden_act': 'gelu',
                        'hidden_dropout_prob': 0.1,
                        'hidden_size': 768,
                        'initializer_range': 0.02,
                        'input_size': 1280,
                        'input_transform_type': 1,
                        'intermediate_size': 3072,
                        'max_position_embeddings': 512,
                        'num_attention_heads': 12,
                        'num_hidden_layers': 12,
                        'obj_pos_id_relative': True,
                        'object_word_embed_mode': 2,
                        'position_padding_idx': -1,
                        'type_vocab_size': 3,
                        'visual_ln': True,
                        'visual_scale_object_init': 0.0,
                        'visual_scale_text_init': 0.0,
                        'visual_size': 768,
                        'vocab_size': 30522,
                        'with_pooler': False,
                        'word_embedding_frozen': False}},
 'NUM_WORKERS_PER_GPU': 4,
 'OUTPUT_PATH': 'checkpoints/./output/vl-bert/refcoco+',
 'RNG_SEED': 12345,
 'SCALES': [600, 1000],
 'TEST': {'BATCH_IMAGES': 4, 'FLIP_PROB': 0, 'SHUFFLE': False, 'TEST_EPOCH': 0},
 'TRAIN': {'ASPECT_GROUPING': True,
           'AUTO_RESUME': True,
           'BATCH_IMAGES': 4,
           'BEGIN_EPOCH': 0,
           'CLIP_GRAD_NORM': 1.0,
           'END_EPOCH': 20,
           'FLIP_PROB': 0.5,
           'FP16': False,
           'FP16_LOSS_SCALE': 128.0,
           'GRAD_ACCUMULATE_STEPS': 2,
           'LOSS_LOGGERS': [('cls_loss', 'ClsLoss')],
           'LR': 8e-07,
           'LR_FACTOR': 0.1,
           'LR_MULT': [],
           'LR_SCHEDULE': 'triangle',
           'LR_STEP': [],
           'MOMENTUM': 0.9,
           'OPTIMIZER': 'AdamW',
           'RESUME': False,
           'SHUFFLE': True,
           'VISUAL_SCALE_CLIP_GRAD_NORM': -1,
           'VISUAL_SCALE_OBJECT_LR_MULT': 1.0,
           'VISUAL_SCALE_TEXT_LR_MULT': 1.0,
           'WARMUP': True,
           'WARMUP_FACTOR': 0.0,
           'WARMUP_METHOD': 'linear',
           'WARMUP_STEPS': 3750,
           'WD': 0.0001},
 'VAL': {'BATCH_IMAGES': 4, 'FLIP_PROB': 0, 'SHUFFLE': False},
 'VAL_FREQUENT': 1}
/fs/classhomes/fall2020/cmsc828w/c828w038/Visual-grounding/VLBERT/VL-BERT/refcoco/../common/backbone/resnet/resnet.py:214: UserWarning: miss keys: ['bn1.num_batches_tracked', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.bn3.num_batches_tracked', 'layer1.0.downsample.1.num_batches_tracked', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.bn2.num_batches_tracked', 'layer1.1.bn3.num_batches_tracked', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.bn2.num_batches_tracked', 'layer1.2.bn3.num_batches_tracked', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.bn3.num_batches_tracked', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.bn2.num_batches_tracked', 'layer2.1.bn3.num_batches_tracked', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.bn2.num_batches_tracked', 'layer2.2.bn3.num_batches_tracked', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.bn2.num_batches_tracked', 'layer2.3.bn3.num_batches_tracked', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.bn3.num_batches_tracked', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.bn2.num_batches_tracked', 'layer3.1.bn3.num_batches_tracked', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.bn2.num_batches_tracked', 'layer3.2.bn3.num_batches_tracked', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.bn2.num_batches_tracked', 'layer3.3.bn3.num_batches_tracked', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.bn2.num_batches_tracked', 'layer3.4.bn3.num_batches_tracked', 'layer3.5.bn1.num_batches_tracked', 'layer3.5.bn2.num_batches_tracked', 'layer3.5.bn3.num_batches_tracked', 'layer3.6.bn1.num_batches_tracked', 'layer3.6.bn2.num_batches_tracked', 'layer3.6.bn3.num_batches_tracked', 'layer3.7.bn1.num_batches_tracked', 'layer3.7.bn2.num_batches_tracked', 'layer3.7.bn3.num_batches_tracked', 'layer3.8.bn1.num_batches_tracked', 'layer3.8.bn2.num_batches_tracked', 'layer3.8.bn3.num_batches_tracked', 'layer3.9.bn1.num_batches_tracked', 'layer3.9.bn2.num_batches_tracked', 'layer3.9.bn3.num_batches_tracked', 'layer3.10.bn1.num_batches_tracked', 'layer3.10.bn2.num_batches_tracked', 'layer3.10.bn3.num_batches_tracked', 'layer3.11.bn1.num_batches_tracked', 'layer3.11.bn2.num_batches_tracked', 'layer3.11.bn3.num_batches_tracked', 'layer3.12.bn1.num_batches_tracked', 'layer3.12.bn2.num_batches_tracked', 'layer3.12.bn3.num_batches_tracked', 'layer3.13.bn1.num_batches_tracked', 'layer3.13.bn2.num_batches_tracked', 'layer3.13.bn3.num_batches_tracked', 'layer3.14.bn1.num_batches_tracked', 'layer3.14.bn2.num_batches_tracked', 'layer3.14.bn3.num_batches_tracked', 'layer3.15.bn1.num_batches_tracked', 'layer3.15.bn2.num_batches_tracked', 'layer3.15.bn3.num_batches_tracked', 'layer3.16.bn1.num_batches_tracked', 'layer3.16.bn2.num_batches_tracked', 'layer3.16.bn3.num_batches_tracked', 'layer3.17.bn1.num_batches_tracked', 'layer3.17.bn2.num_batches_tracked', 'layer3.17.bn3.num_batches_tracked', 'layer3.18.bn1.num_batches_tracked', 'layer3.18.bn2.num_batches_tracked', 'layer3.18.bn3.num_batches_tracked', 'layer3.19.bn1.num_batches_tracked', 'layer3.19.bn2.num_batches_tracked', 'layer3.19.bn3.num_batches_tracked', 'layer3.20.bn1.num_batches_tracked', 'layer3.20.bn2.num_batches_tracked', 'layer3.20.bn3.num_batches_tracked', 'layer3.21.bn1.num_batches_tracked', 'layer3.21.bn2.num_batches_tracked', 'layer3.21.bn3.num_batches_tracked', 'layer3.22.bn1.num_batches_tracked', 'layer3.22.bn2.num_batches_tracked', 'layer3.22.bn3.num_batches_tracked']
  warnings.warn('miss keys: {}'.format(miss_keys))
Warnings: Unexpected keys: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias'].
native distributed, size: 1, rank: 0, local rank: 0
>> Trainable Parameters:
------------------------------------------------------------------------------------------------------------------------------------
|Name                                                                        |Dtype            |Shape                 |#Params     |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.0.conv1.weight                      |torch.float32    |(128, 256, 1, 1)      |32768       |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.0.conv2.weight                      |torch.float32    |(128, 128, 3, 3)      |147456      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.0.conv3.weight                      |torch.float32    |(512, 128, 1, 1)      |65536       |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.0.downsample.0.weight               |torch.float32    |(512, 256, 1, 1)      |131072      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.1.conv1.weight                      |torch.float32    |(128, 512, 1, 1)      |65536       |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.1.conv2.weight                      |torch.float32    |(128, 128, 3, 3)      |147456      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.1.conv3.weight                      |torch.float32    |(512, 128, 1, 1)      |65536       |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.2.conv1.weight                      |torch.float32    |(128, 512, 1, 1)      |65536       |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.2.conv2.weight                      |torch.float32    |(128, 128, 3, 3)      |147456      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.2.conv3.weight                      |torch.float32    |(512, 128, 1, 1)      |65536       |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.3.conv1.weight                      |torch.float32    |(128, 512, 1, 1)      |65536       |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.3.conv2.weight                      |torch.float32    |(128, 128, 3, 3)      |147456      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer2.3.conv3.weight                      |torch.float32    |(512, 128, 1, 1)      |65536       |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.0.conv1.weight                      |torch.float32    |(256, 512, 1, 1)      |131072      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.0.conv2.weight                      |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.0.conv3.weight                      |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.0.downsample.0.weight               |torch.float32    |(1024, 512, 1, 1)     |524288      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.1.conv1.weight                      |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.1.conv2.weight                      |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.1.conv3.weight                      |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.2.conv1.weight                      |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.2.conv2.weight                      |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.2.conv3.weight                      |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.3.conv1.weight                      |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.3.conv2.weight                      |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.3.conv3.weight                      |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.4.conv1.weight                      |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.4.conv2.weight                      |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.4.conv3.weight                      |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.5.conv1.weight                      |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.5.conv2.weight                      |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.5.conv3.weight                      |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.6.conv1.weight                      |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.6.conv2.weight                      |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.6.conv3.weight                      |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.7.conv1.weight                      |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.7.conv2.weight                      |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.7.conv3.weight                      |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.8.conv1.weight                      |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.8.conv2.weight                      |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.8.conv3.weight                      |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.9.conv1.weight                      |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.9.conv2.weight                      |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.9.conv3.weight                      |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.10.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.10.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.10.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.11.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.11.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.11.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.12.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.12.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.12.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.13.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.13.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.13.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.14.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.14.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.14.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.15.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.15.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.15.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.16.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.16.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.16.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.17.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.17.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.17.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.18.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.18.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.18.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.19.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.19.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.19.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.20.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.20.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.20.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.21.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.21.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.21.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.22.conv1.weight                     |torch.float32    |(256, 1024, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.22.conv2.weight                     |torch.float32    |(256, 256, 3, 3)      |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.backbone.layer3.22.conv3.weight                     |torch.float32    |(1024, 256, 1, 1)     |262144      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.roi_head_feature_extractor.0.conv1.weight           |torch.float32    |(512, 1024, 1, 1)     |524288      |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.roi_head_feature_extractor.0.conv2.weight           |torch.float32    |(512, 512, 3, 3)      |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.roi_head_feature_extractor.0.conv3.weight           |torch.float32    |(2048, 512, 1, 1)     |1048576     |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.roi_head_feature_extractor.0.downsample.0.weight    |torch.float32    |(2048, 1024, 1, 1)    |2097152     |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.roi_head_feature_extractor.1.conv1.weight           |torch.float32    |(512, 2048, 1, 1)     |1048576     |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.roi_head_feature_extractor.1.conv2.weight           |torch.float32    |(512, 512, 3, 3)      |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.roi_head_feature_extractor.1.conv3.weight           |torch.float32    |(2048, 512, 1, 1)     |1048576     |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.roi_head_feature_extractor.2.conv1.weight           |torch.float32    |(512, 2048, 1, 1)     |1048576     |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.roi_head_feature_extractor.2.conv2.weight           |torch.float32    |(512, 512, 3, 3)      |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.roi_head_feature_extractor.2.conv3.weight           |torch.float32    |(2048, 512, 1, 1)     |1048576     |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.obj_downsample.1.weight                             |torch.float32    |(768, 4096)           |3145728     |
------------------------------------------------------------------------------------------------------------------------------------
|image_feature_extractor.obj_downsample.1.bias                               |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|object_linguistic_embeddings.weight                                         |torch.float32    |(1, 768)              |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.word_embeddings.weight                                               |torch.float32    |(30522, 768)          |23440896    |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.end_embedding.weight                                                 |torch.float32    |(1, 768)              |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.position_embeddings.weight                                           |torch.float32    |(512, 768)            |393216      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.token_type_embeddings.weight                                         |torch.float32    |(3, 768)              |2304        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.embedding_LayerNorm.weight                                           |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.embedding_LayerNorm.bias                                             |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.visual_ln_text.weight                                                |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.visual_ln_text.bias                                                  |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.visual_ln_object.weight                                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.visual_ln_object.bias                                                |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.attention.self.query.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.attention.self.query.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.attention.self.key.weight                            |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.attention.self.key.bias                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.attention.self.value.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.attention.self.value.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.attention.output.dense.weight                        |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.attention.output.dense.bias                          |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.attention.output.LayerNorm.weight                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.attention.output.LayerNorm.bias                      |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.intermediate.dense.weight                            |torch.float32    |(3072, 768)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.intermediate.dense.bias                              |torch.float32    |(3072,)               |3072        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.output.dense.weight                                  |torch.float32    |(768, 3072)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.output.dense.bias                                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.output.LayerNorm.weight                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.0.output.LayerNorm.bias                                |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.attention.self.query.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.attention.self.query.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.attention.self.key.weight                            |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.attention.self.key.bias                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.attention.self.value.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.attention.self.value.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.attention.output.dense.weight                        |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.attention.output.dense.bias                          |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.attention.output.LayerNorm.weight                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.attention.output.LayerNorm.bias                      |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.intermediate.dense.weight                            |torch.float32    |(3072, 768)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.intermediate.dense.bias                              |torch.float32    |(3072,)               |3072        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.output.dense.weight                                  |torch.float32    |(768, 3072)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.output.dense.bias                                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.output.LayerNorm.weight                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.1.output.LayerNorm.bias                                |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.attention.self.query.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.attention.self.query.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.attention.self.key.weight                            |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.attention.self.key.bias                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.attention.self.value.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.attention.self.value.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.attention.output.dense.weight                        |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.attention.output.dense.bias                          |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.attention.output.LayerNorm.weight                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.attention.output.LayerNorm.bias                      |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.intermediate.dense.weight                            |torch.float32    |(3072, 768)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.intermediate.dense.bias                              |torch.float32    |(3072,)               |3072        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.output.dense.weight                                  |torch.float32    |(768, 3072)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.output.dense.bias                                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.output.LayerNorm.weight                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.2.output.LayerNorm.bias                                |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.attention.self.query.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.attention.self.query.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.attention.self.key.weight                            |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.attention.self.key.bias                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.attention.self.value.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.attention.self.value.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.attention.output.dense.weight                        |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.attention.output.dense.bias                          |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.attention.output.LayerNorm.weight                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.attention.output.LayerNorm.bias                      |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.intermediate.dense.weight                            |torch.float32    |(3072, 768)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.intermediate.dense.bias                              |torch.float32    |(3072,)               |3072        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.output.dense.weight                                  |torch.float32    |(768, 3072)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.output.dense.bias                                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.output.LayerNorm.weight                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.3.output.LayerNorm.bias                                |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.attention.self.query.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.attention.self.query.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.attention.self.key.weight                            |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.attention.self.key.bias                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.attention.self.value.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.attention.self.value.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.attention.output.dense.weight                        |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.attention.output.dense.bias                          |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.attention.output.LayerNorm.weight                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.attention.output.LayerNorm.bias                      |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.intermediate.dense.weight                            |torch.float32    |(3072, 768)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.intermediate.dense.bias                              |torch.float32    |(3072,)               |3072        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.output.dense.weight                                  |torch.float32    |(768, 3072)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.output.dense.bias                                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.output.LayerNorm.weight                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.4.output.LayerNorm.bias                                |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.attention.self.query.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.attention.self.query.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.attention.self.key.weight                            |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.attention.self.key.bias                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.attention.self.value.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.attention.self.value.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.attention.output.dense.weight                        |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.attention.output.dense.bias                          |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.attention.output.LayerNorm.weight                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.attention.output.LayerNorm.bias                      |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.intermediate.dense.weight                            |torch.float32    |(3072, 768)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.intermediate.dense.bias                              |torch.float32    |(3072,)               |3072        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.output.dense.weight                                  |torch.float32    |(768, 3072)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.output.dense.bias                                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.output.LayerNorm.weight                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.5.output.LayerNorm.bias                                |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.attention.self.query.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.attention.self.query.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.attention.self.key.weight                            |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.attention.self.key.bias                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.attention.self.value.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.attention.self.value.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.attention.output.dense.weight                        |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.attention.output.dense.bias                          |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.attention.output.LayerNorm.weight                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.attention.output.LayerNorm.bias                      |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.intermediate.dense.weight                            |torch.float32    |(3072, 768)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.intermediate.dense.bias                              |torch.float32    |(3072,)               |3072        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.output.dense.weight                                  |torch.float32    |(768, 3072)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.output.dense.bias                                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.output.LayerNorm.weight                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.6.output.LayerNorm.bias                                |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.attention.self.query.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.attention.self.query.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.attention.self.key.weight                            |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.attention.self.key.bias                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.attention.self.value.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.attention.self.value.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.attention.output.dense.weight                        |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.attention.output.dense.bias                          |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.attention.output.LayerNorm.weight                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.attention.output.LayerNorm.bias                      |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.intermediate.dense.weight                            |torch.float32    |(3072, 768)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.intermediate.dense.bias                              |torch.float32    |(3072,)               |3072        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.output.dense.weight                                  |torch.float32    |(768, 3072)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.output.dense.bias                                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.output.LayerNorm.weight                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.7.output.LayerNorm.bias                                |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.attention.self.query.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.attention.self.query.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.attention.self.key.weight                            |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.attention.self.key.bias                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.attention.self.value.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.attention.self.value.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.attention.output.dense.weight                        |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.attention.output.dense.bias                          |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.attention.output.LayerNorm.weight                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.attention.output.LayerNorm.bias                      |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.intermediate.dense.weight                            |torch.float32    |(3072, 768)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.intermediate.dense.bias                              |torch.float32    |(3072,)               |3072        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.output.dense.weight                                  |torch.float32    |(768, 3072)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.output.dense.bias                                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.output.LayerNorm.weight                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.8.output.LayerNorm.bias                                |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.attention.self.query.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.attention.self.query.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.attention.self.key.weight                            |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.attention.self.key.bias                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.attention.self.value.weight                          |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.attention.self.value.bias                            |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.attention.output.dense.weight                        |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.attention.output.dense.bias                          |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.attention.output.LayerNorm.weight                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.attention.output.LayerNorm.bias                      |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.intermediate.dense.weight                            |torch.float32    |(3072, 768)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.intermediate.dense.bias                              |torch.float32    |(3072,)               |3072        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.output.dense.weight                                  |torch.float32    |(768, 3072)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.output.dense.bias                                    |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.output.LayerNorm.weight                              |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.9.output.LayerNorm.bias                                |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.attention.self.query.weight                         |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.attention.self.query.bias                           |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.attention.self.key.weight                           |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.attention.self.key.bias                             |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.attention.self.value.weight                         |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.attention.self.value.bias                           |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.attention.output.dense.weight                       |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.attention.output.dense.bias                         |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.attention.output.LayerNorm.weight                   |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.attention.output.LayerNorm.bias                     |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.intermediate.dense.weight                           |torch.float32    |(3072, 768)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.intermediate.dense.bias                             |torch.float32    |(3072,)               |3072        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.output.dense.weight                                 |torch.float32    |(768, 3072)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.output.dense.bias                                   |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.output.LayerNorm.weight                             |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.10.output.LayerNorm.bias                               |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.attention.self.query.weight                         |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.attention.self.query.bias                           |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.attention.self.key.weight                           |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.attention.self.key.bias                             |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.attention.self.value.weight                         |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.attention.self.value.bias                           |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.attention.output.dense.weight                       |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.attention.output.dense.bias                         |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.attention.output.LayerNorm.weight                   |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.attention.output.LayerNorm.bias                     |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.intermediate.dense.weight                           |torch.float32    |(3072, 768)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.intermediate.dense.bias                             |torch.float32    |(3072,)               |3072        |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.output.dense.weight                                 |torch.float32    |(768, 3072)           |2359296     |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.output.dense.bias                                   |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.output.LayerNorm.weight                             |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|vlbert.encoder.layer.11.output.LayerNorm.bias                               |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|final_mlp.0.dense.weight                                                    |torch.float32    |(768, 768)            |589824      |
------------------------------------------------------------------------------------------------------------------------------------
|final_mlp.0.dense.bias                                                      |torch.float32    |(768,)                |768         |
------------------------------------------------------------------------------------------------------------------------------------
|final_mlp.2.weight                                                          |torch.float32    |(1, 768)              |768         |
------------------------------------------------------------------------------------------------------------------------------------
|final_mlp.2.bias                                                            |torch.float32    |(1,)                  |1           |
------------------------------------------------------------------------------------------------------------------------------------
>> # TrainableParams:       	154.81	M
>> # NonTrainableParams:    	0.33	M
>> # TotalParams:           	155.14	M
loading annotations into memory...
Done (t=10.21s)
creating index...
index created!
loading dataset refcoco+ into memory...
creating index...
index created.
DONE (t=5.80s)
cached database ignored.
loading database of split train...
Done (t=1.82s)
caching database to ./cache/refcoco+_boxes_gt_train.pkl...
Done (t=0.45s)
grouping aspect...
Done (t=0.04s)
loading annotations into memory...
Done (t=10.76s)
creating index...
index created!
loading dataset refcoco+ into memory...
creating index...
index created.
DONE (t=7.51s)
cached database ignored.
loading database of split val...
Done (t=0.04s)
caching database to ./cache/refcoco+_boxes_gt_val.pkl...
Done (t=0.04s)
[Partial Load] partial load state dict of keys: dict_keys(['module.image_feature_extractor.backbone.conv1.weight', 'module.image_feature_extractor.backbone.bn1.weight', 'module.image_feature_extractor.backbone.bn1.bias', 'module.image_feature_extractor.backbone.bn1.running_mean', 'module.image_feature_extractor.backbone.bn1.running_var', 'module.image_feature_extractor.backbone.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer1.0.conv1.weight', 'module.image_feature_extractor.backbone.layer1.0.bn1.weight', 'module.image_feature_extractor.backbone.layer1.0.bn1.bias', 'module.image_feature_extractor.backbone.layer1.0.bn1.running_mean', 'module.image_feature_extractor.backbone.layer1.0.bn1.running_var', 'module.image_feature_extractor.backbone.layer1.0.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer1.0.conv2.weight', 'module.image_feature_extractor.backbone.layer1.0.bn2.weight', 'module.image_feature_extractor.backbone.layer1.0.bn2.bias', 'module.image_feature_extractor.backbone.layer1.0.bn2.running_mean', 'module.image_feature_extractor.backbone.layer1.0.bn2.running_var', 'module.image_feature_extractor.backbone.layer1.0.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer1.0.conv3.weight', 'module.image_feature_extractor.backbone.layer1.0.bn3.weight', 'module.image_feature_extractor.backbone.layer1.0.bn3.bias', 'module.image_feature_extractor.backbone.layer1.0.bn3.running_mean', 'module.image_feature_extractor.backbone.layer1.0.bn3.running_var', 'module.image_feature_extractor.backbone.layer1.0.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer1.0.downsample.0.weight', 'module.image_feature_extractor.backbone.layer1.0.downsample.1.weight', 'module.image_feature_extractor.backbone.layer1.0.downsample.1.bias', 'module.image_feature_extractor.backbone.layer1.0.downsample.1.running_mean', 'module.image_feature_extractor.backbone.layer1.0.downsample.1.running_var', 'module.image_feature_extractor.backbone.layer1.0.downsample.1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer1.1.conv1.weight', 'module.image_feature_extractor.backbone.layer1.1.bn1.weight', 'module.image_feature_extractor.backbone.layer1.1.bn1.bias', 'module.image_feature_extractor.backbone.layer1.1.bn1.running_mean', 'module.image_feature_extractor.backbone.layer1.1.bn1.running_var', 'module.image_feature_extractor.backbone.layer1.1.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer1.1.conv2.weight', 'module.image_feature_extractor.backbone.layer1.1.bn2.weight', 'module.image_feature_extractor.backbone.layer1.1.bn2.bias', 'module.image_feature_extractor.backbone.layer1.1.bn2.running_mean', 'module.image_feature_extractor.backbone.layer1.1.bn2.running_var', 'module.image_feature_extractor.backbone.layer1.1.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer1.1.conv3.weight', 'module.image_feature_extractor.backbone.layer1.1.bn3.weight', 'module.image_feature_extractor.backbone.layer1.1.bn3.bias', 'module.image_feature_extractor.backbone.layer1.1.bn3.running_mean', 'module.image_feature_extractor.backbone.layer1.1.bn3.running_var', 'module.image_feature_extractor.backbone.layer1.1.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer1.2.conv1.weight', 'module.image_feature_extractor.backbone.layer1.2.bn1.weight', 'module.image_feature_extractor.backbone.layer1.2.bn1.bias', 'module.image_feature_extractor.backbone.layer1.2.bn1.running_mean', 'module.image_feature_extractor.backbone.layer1.2.bn1.running_var', 'module.image_feature_extractor.backbone.layer1.2.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer1.2.conv2.weight', 'module.image_feature_extractor.backbone.layer1.2.bn2.weight', 'module.image_feature_extractor.backbone.layer1.2.bn2.bias', 'module.image_feature_extractor.backbone.layer1.2.bn2.running_mean', 'module.image_feature_extractor.backbone.layer1.2.bn2.running_var', 'module.image_feature_extractor.backbone.layer1.2.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer1.2.conv3.weight', 'module.image_feature_extractor.backbone.layer1.2.bn3.weight', 'module.image_feature_extractor.backbone.layer1.2.bn3.bias', 'module.image_feature_extractor.backbone.layer1.2.bn3.running_mean', 'module.image_feature_extractor.backbone.layer1.2.bn3.running_var', 'module.image_feature_extractor.backbone.layer1.2.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.0.conv1.weight', 'module.image_feature_extractor.backbone.layer2.0.bn1.weight', 'module.image_feature_extractor.backbone.layer2.0.bn1.bias', 'module.image_feature_extractor.backbone.layer2.0.bn1.running_mean', 'module.image_feature_extractor.backbone.layer2.0.bn1.running_var', 'module.image_feature_extractor.backbone.layer2.0.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.0.conv2.weight', 'module.image_feature_extractor.backbone.layer2.0.bn2.weight', 'module.image_feature_extractor.backbone.layer2.0.bn2.bias', 'module.image_feature_extractor.backbone.layer2.0.bn2.running_mean', 'module.image_feature_extractor.backbone.layer2.0.bn2.running_var', 'module.image_feature_extractor.backbone.layer2.0.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.0.conv3.weight', 'module.image_feature_extractor.backbone.layer2.0.bn3.weight', 'module.image_feature_extractor.backbone.layer2.0.bn3.bias', 'module.image_feature_extractor.backbone.layer2.0.bn3.running_mean', 'module.image_feature_extractor.backbone.layer2.0.bn3.running_var', 'module.image_feature_extractor.backbone.layer2.0.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.0.downsample.0.weight', 'module.image_feature_extractor.backbone.layer2.0.downsample.1.weight', 'module.image_feature_extractor.backbone.layer2.0.downsample.1.bias', 'module.image_feature_extractor.backbone.layer2.0.downsample.1.running_mean', 'module.image_feature_extractor.backbone.layer2.0.downsample.1.running_var', 'module.image_feature_extractor.backbone.layer2.0.downsample.1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.1.conv1.weight', 'module.image_feature_extractor.backbone.layer2.1.bn1.weight', 'module.image_feature_extractor.backbone.layer2.1.bn1.bias', 'module.image_feature_extractor.backbone.layer2.1.bn1.running_mean', 'module.image_feature_extractor.backbone.layer2.1.bn1.running_var', 'module.image_feature_extractor.backbone.layer2.1.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.1.conv2.weight', 'module.image_feature_extractor.backbone.layer2.1.bn2.weight', 'module.image_feature_extractor.backbone.layer2.1.bn2.bias', 'module.image_feature_extractor.backbone.layer2.1.bn2.running_mean', 'module.image_feature_extractor.backbone.layer2.1.bn2.running_var', 'module.image_feature_extractor.backbone.layer2.1.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.1.conv3.weight', 'module.image_feature_extractor.backbone.layer2.1.bn3.weight', 'module.image_feature_extractor.backbone.layer2.1.bn3.bias', 'module.image_feature_extractor.backbone.layer2.1.bn3.running_mean', 'module.image_feature_extractor.backbone.layer2.1.bn3.running_var', 'module.image_feature_extractor.backbone.layer2.1.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.2.conv1.weight', 'module.image_feature_extractor.backbone.layer2.2.bn1.weight', 'module.image_feature_extractor.backbone.layer2.2.bn1.bias', 'module.image_feature_extractor.backbone.layer2.2.bn1.running_mean', 'module.image_feature_extractor.backbone.layer2.2.bn1.running_var', 'module.image_feature_extractor.backbone.layer2.2.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.2.conv2.weight', 'module.image_feature_extractor.backbone.layer2.2.bn2.weight', 'module.image_feature_extractor.backbone.layer2.2.bn2.bias', 'module.image_feature_extractor.backbone.layer2.2.bn2.running_mean', 'module.image_feature_extractor.backbone.layer2.2.bn2.running_var', 'module.image_feature_extractor.backbone.layer2.2.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.2.conv3.weight', 'module.image_feature_extractor.backbone.layer2.2.bn3.weight', 'module.image_feature_extractor.backbone.layer2.2.bn3.bias', 'module.image_feature_extractor.backbone.layer2.2.bn3.running_mean', 'module.image_feature_extractor.backbone.layer2.2.bn3.running_var', 'module.image_feature_extractor.backbone.layer2.2.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.3.conv1.weight', 'module.image_feature_extractor.backbone.layer2.3.bn1.weight', 'module.image_feature_extractor.backbone.layer2.3.bn1.bias', 'module.image_feature_extractor.backbone.layer2.3.bn1.running_mean', 'module.image_feature_extractor.backbone.layer2.3.bn1.running_var', 'module.image_feature_extractor.backbone.layer2.3.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.3.conv2.weight', 'module.image_feature_extractor.backbone.layer2.3.bn2.weight', 'module.image_feature_extractor.backbone.layer2.3.bn2.bias', 'module.image_feature_extractor.backbone.layer2.3.bn2.running_mean', 'module.image_feature_extractor.backbone.layer2.3.bn2.running_var', 'module.image_feature_extractor.backbone.layer2.3.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer2.3.conv3.weight', 'module.image_feature_extractor.backbone.layer2.3.bn3.weight', 'module.image_feature_extractor.backbone.layer2.3.bn3.bias', 'module.image_feature_extractor.backbone.layer2.3.bn3.running_mean', 'module.image_feature_extractor.backbone.layer2.3.bn3.running_var', 'module.image_feature_extractor.backbone.layer2.3.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.0.conv1.weight', 'module.image_feature_extractor.backbone.layer3.0.bn1.weight', 'module.image_feature_extractor.backbone.layer3.0.bn1.bias', 'module.image_feature_extractor.backbone.layer3.0.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.0.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.0.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.0.conv2.weight', 'module.image_feature_extractor.backbone.layer3.0.bn2.weight', 'module.image_feature_extractor.backbone.layer3.0.bn2.bias', 'module.image_feature_extractor.backbone.layer3.0.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.0.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.0.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.0.conv3.weight', 'module.image_feature_extractor.backbone.layer3.0.bn3.weight', 'module.image_feature_extractor.backbone.layer3.0.bn3.bias', 'module.image_feature_extractor.backbone.layer3.0.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.0.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.0.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.0.downsample.0.weight', 'module.image_feature_extractor.backbone.layer3.0.downsample.1.weight', 'module.image_feature_extractor.backbone.layer3.0.downsample.1.bias', 'module.image_feature_extractor.backbone.layer3.0.downsample.1.running_mean', 'module.image_feature_extractor.backbone.layer3.0.downsample.1.running_var', 'module.image_feature_extractor.backbone.layer3.0.downsample.1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.1.conv1.weight', 'module.image_feature_extractor.backbone.layer3.1.bn1.weight', 'module.image_feature_extractor.backbone.layer3.1.bn1.bias', 'module.image_feature_extractor.backbone.layer3.1.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.1.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.1.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.1.conv2.weight', 'module.image_feature_extractor.backbone.layer3.1.bn2.weight', 'module.image_feature_extractor.backbone.layer3.1.bn2.bias', 'module.image_feature_extractor.backbone.layer3.1.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.1.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.1.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.1.conv3.weight', 'module.image_feature_extractor.backbone.layer3.1.bn3.weight', 'module.image_feature_extractor.backbone.layer3.1.bn3.bias', 'module.image_feature_extractor.backbone.layer3.1.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.1.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.1.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.2.conv1.weight', 'module.image_feature_extractor.backbone.layer3.2.bn1.weight', 'module.image_feature_extractor.backbone.layer3.2.bn1.bias', 'module.image_feature_extractor.backbone.layer3.2.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.2.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.2.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.2.conv2.weight', 'module.image_feature_extractor.backbone.layer3.2.bn2.weight', 'module.image_feature_extractor.backbone.layer3.2.bn2.bias', 'module.image_feature_extractor.backbone.layer3.2.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.2.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.2.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.2.conv3.weight', 'module.image_feature_extractor.backbone.layer3.2.bn3.weight', 'module.image_feature_extractor.backbone.layer3.2.bn3.bias', 'module.image_feature_extractor.backbone.layer3.2.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.2.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.2.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.3.conv1.weight', 'module.image_feature_extractor.backbone.layer3.3.bn1.weight', 'module.image_feature_extractor.backbone.layer3.3.bn1.bias', 'module.image_feature_extractor.backbone.layer3.3.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.3.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.3.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.3.conv2.weight', 'module.image_feature_extractor.backbone.layer3.3.bn2.weight', 'module.image_feature_extractor.backbone.layer3.3.bn2.bias', 'module.image_feature_extractor.backbone.layer3.3.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.3.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.3.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.3.conv3.weight', 'module.image_feature_extractor.backbone.layer3.3.bn3.weight', 'module.image_feature_extractor.backbone.layer3.3.bn3.bias', 'module.image_feature_extractor.backbone.layer3.3.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.3.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.3.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.4.conv1.weight', 'module.image_feature_extractor.backbone.layer3.4.bn1.weight', 'module.image_feature_extractor.backbone.layer3.4.bn1.bias', 'module.image_feature_extractor.backbone.layer3.4.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.4.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.4.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.4.conv2.weight', 'module.image_feature_extractor.backbone.layer3.4.bn2.weight', 'module.image_feature_extractor.backbone.layer3.4.bn2.bias', 'module.image_feature_extractor.backbone.layer3.4.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.4.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.4.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.4.conv3.weight', 'module.image_feature_extractor.backbone.layer3.4.bn3.weight', 'module.image_feature_extractor.backbone.layer3.4.bn3.bias', 'module.image_feature_extractor.backbone.layer3.4.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.4.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.4.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.5.conv1.weight', 'module.image_feature_extractor.backbone.layer3.5.bn1.weight', 'module.image_feature_extractor.backbone.layer3.5.bn1.bias', 'module.image_feature_extractor.backbone.layer3.5.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.5.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.5.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.5.conv2.weight', 'module.image_feature_extractor.backbone.layer3.5.bn2.weight', 'module.image_feature_extractor.backbone.layer3.5.bn2.bias', 'module.image_feature_extractor.backbone.layer3.5.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.5.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.5.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.5.conv3.weight', 'module.image_feature_extractor.backbone.layer3.5.bn3.weight', 'module.image_feature_extractor.backbone.layer3.5.bn3.bias', 'module.image_feature_extractor.backbone.layer3.5.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.5.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.5.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.6.conv1.weight', 'module.image_feature_extractor.backbone.layer3.6.bn1.weight', 'module.image_feature_extractor.backbone.layer3.6.bn1.bias', 'module.image_feature_extractor.backbone.layer3.6.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.6.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.6.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.6.conv2.weight', 'module.image_feature_extractor.backbone.layer3.6.bn2.weight', 'module.image_feature_extractor.backbone.layer3.6.bn2.bias', 'module.image_feature_extractor.backbone.layer3.6.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.6.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.6.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.6.conv3.weight', 'module.image_feature_extractor.backbone.layer3.6.bn3.weight', 'module.image_feature_extractor.backbone.layer3.6.bn3.bias', 'module.image_feature_extractor.backbone.layer3.6.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.6.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.6.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.7.conv1.weight', 'module.image_feature_extractor.backbone.layer3.7.bn1.weight', 'module.image_feature_extractor.backbone.layer3.7.bn1.bias', 'module.image_feature_extractor.backbone.layer3.7.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.7.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.7.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.7.conv2.weight', 'module.image_feature_extractor.backbone.layer3.7.bn2.weight', 'module.image_feature_extractor.backbone.layer3.7.bn2.bias', 'module.image_feature_extractor.backbone.layer3.7.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.7.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.7.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.7.conv3.weight', 'module.image_feature_extractor.backbone.layer3.7.bn3.weight', 'module.image_feature_extractor.backbone.layer3.7.bn3.bias', 'module.image_feature_extractor.backbone.layer3.7.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.7.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.7.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.8.conv1.weight', 'module.image_feature_extractor.backbone.layer3.8.bn1.weight', 'module.image_feature_extractor.backbone.layer3.8.bn1.bias', 'module.image_feature_extractor.backbone.layer3.8.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.8.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.8.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.8.conv2.weight', 'module.image_feature_extractor.backbone.layer3.8.bn2.weight', 'module.image_feature_extractor.backbone.layer3.8.bn2.bias', 'module.image_feature_extractor.backbone.layer3.8.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.8.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.8.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.8.conv3.weight', 'module.image_feature_extractor.backbone.layer3.8.bn3.weight', 'module.image_feature_extractor.backbone.layer3.8.bn3.bias', 'module.image_feature_extractor.backbone.layer3.8.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.8.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.8.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.9.conv1.weight', 'module.image_feature_extractor.backbone.layer3.9.bn1.weight', 'module.image_feature_extractor.backbone.layer3.9.bn1.bias', 'module.image_feature_extractor.backbone.layer3.9.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.9.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.9.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.9.conv2.weight', 'module.image_feature_extractor.backbone.layer3.9.bn2.weight', 'module.image_feature_extractor.backbone.layer3.9.bn2.bias', 'module.image_feature_extractor.backbone.layer3.9.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.9.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.9.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.9.conv3.weight', 'module.image_feature_extractor.backbone.layer3.9.bn3.weight', 'module.image_feature_extractor.backbone.layer3.9.bn3.bias', 'module.image_feature_extractor.backbone.layer3.9.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.9.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.9.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.10.conv1.weight', 'module.image_feature_extractor.backbone.layer3.10.bn1.weight', 'module.image_feature_extractor.backbone.layer3.10.bn1.bias', 'module.image_feature_extractor.backbone.layer3.10.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.10.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.10.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.10.conv2.weight', 'module.image_feature_extractor.backbone.layer3.10.bn2.weight', 'module.image_feature_extractor.backbone.layer3.10.bn2.bias', 'module.image_feature_extractor.backbone.layer3.10.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.10.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.10.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.10.conv3.weight', 'module.image_feature_extractor.backbone.layer3.10.bn3.weight', 'module.image_feature_extractor.backbone.layer3.10.bn3.bias', 'module.image_feature_extractor.backbone.layer3.10.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.10.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.10.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.11.conv1.weight', 'module.image_feature_extractor.backbone.layer3.11.bn1.weight', 'module.image_feature_extractor.backbone.layer3.11.bn1.bias', 'module.image_feature_extractor.backbone.layer3.11.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.11.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.11.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.11.conv2.weight', 'module.image_feature_extractor.backbone.layer3.11.bn2.weight', 'module.image_feature_extractor.backbone.layer3.11.bn2.bias', 'module.image_feature_extractor.backbone.layer3.11.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.11.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.11.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.11.conv3.weight', 'module.image_feature_extractor.backbone.layer3.11.bn3.weight', 'module.image_feature_extractor.backbone.layer3.11.bn3.bias', 'module.image_feature_extractor.backbone.layer3.11.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.11.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.11.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.12.conv1.weight', 'module.image_feature_extractor.backbone.layer3.12.bn1.weight', 'module.image_feature_extractor.backbone.layer3.12.bn1.bias', 'module.image_feature_extractor.backbone.layer3.12.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.12.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.12.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.12.conv2.weight', 'module.image_feature_extractor.backbone.layer3.12.bn2.weight', 'module.image_feature_extractor.backbone.layer3.12.bn2.bias', 'module.image_feature_extractor.backbone.layer3.12.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.12.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.12.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.12.conv3.weight', 'module.image_feature_extractor.backbone.layer3.12.bn3.weight', 'module.image_feature_extractor.backbone.layer3.12.bn3.bias', 'module.image_feature_extractor.backbone.layer3.12.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.12.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.12.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.13.conv1.weight', 'module.image_feature_extractor.backbone.layer3.13.bn1.weight', 'module.image_feature_extractor.backbone.layer3.13.bn1.bias', 'module.image_feature_extractor.backbone.layer3.13.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.13.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.13.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.13.conv2.weight', 'module.image_feature_extractor.backbone.layer3.13.bn2.weight', 'module.image_feature_extractor.backbone.layer3.13.bn2.bias', 'module.image_feature_extractor.backbone.layer3.13.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.13.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.13.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.13.conv3.weight', 'module.image_feature_extractor.backbone.layer3.13.bn3.weight', 'module.image_feature_extractor.backbone.layer3.13.bn3.bias', 'module.image_feature_extractor.backbone.layer3.13.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.13.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.13.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.14.conv1.weight', 'module.image_feature_extractor.backbone.layer3.14.bn1.weight', 'module.image_feature_extractor.backbone.layer3.14.bn1.bias', 'module.image_feature_extractor.backbone.layer3.14.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.14.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.14.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.14.conv2.weight', 'module.image_feature_extractor.backbone.layer3.14.bn2.weight', 'module.image_feature_extractor.backbone.layer3.14.bn2.bias', 'module.image_feature_extractor.backbone.layer3.14.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.14.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.14.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.14.conv3.weight', 'module.image_feature_extractor.backbone.layer3.14.bn3.weight', 'module.image_feature_extractor.backbone.layer3.14.bn3.bias', 'module.image_feature_extractor.backbone.layer3.14.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.14.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.14.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.15.conv1.weight', 'module.image_feature_extractor.backbone.layer3.15.bn1.weight', 'module.image_feature_extractor.backbone.layer3.15.bn1.bias', 'module.image_feature_extractor.backbone.layer3.15.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.15.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.15.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.15.conv2.weight', 'module.image_feature_extractor.backbone.layer3.15.bn2.weight', 'module.image_feature_extractor.backbone.layer3.15.bn2.bias', 'module.image_feature_extractor.backbone.layer3.15.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.15.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.15.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.15.conv3.weight', 'module.image_feature_extractor.backbone.layer3.15.bn3.weight', 'module.image_feature_extractor.backbone.layer3.15.bn3.bias', 'module.image_feature_extractor.backbone.layer3.15.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.15.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.15.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.16.conv1.weight', 'module.image_feature_extractor.backbone.layer3.16.bn1.weight', 'module.image_feature_extractor.backbone.layer3.16.bn1.bias', 'module.image_feature_extractor.backbone.layer3.16.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.16.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.16.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.16.conv2.weight', 'module.image_feature_extractor.backbone.layer3.16.bn2.weight', 'module.image_feature_extractor.backbone.layer3.16.bn2.bias', 'module.image_feature_extractor.backbone.layer3.16.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.16.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.16.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.16.conv3.weight', 'module.image_feature_extractor.backbone.layer3.16.bn3.weight', 'module.image_feature_extractor.backbone.layer3.16.bn3.bias', 'module.image_feature_extractor.backbone.layer3.16.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.16.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.16.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.17.conv1.weight', 'module.image_feature_extractor.backbone.layer3.17.bn1.weight', 'module.image_feature_extractor.backbone.layer3.17.bn1.bias', 'module.image_feature_extractor.backbone.layer3.17.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.17.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.17.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.17.conv2.weight', 'module.image_feature_extractor.backbone.layer3.17.bn2.weight', 'module.image_feature_extractor.backbone.layer3.17.bn2.bias', 'module.image_feature_extractor.backbone.layer3.17.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.17.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.17.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.17.conv3.weight', 'module.image_feature_extractor.backbone.layer3.17.bn3.weight', 'module.image_feature_extractor.backbone.layer3.17.bn3.bias', 'module.image_feature_extractor.backbone.layer3.17.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.17.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.17.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.18.conv1.weight', 'module.image_feature_extractor.backbone.layer3.18.bn1.weight', 'module.image_feature_extractor.backbone.layer3.18.bn1.bias', 'module.image_feature_extractor.backbone.layer3.18.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.18.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.18.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.18.conv2.weight', 'module.image_feature_extractor.backbone.layer3.18.bn2.weight', 'module.image_feature_extractor.backbone.layer3.18.bn2.bias', 'module.image_feature_extractor.backbone.layer3.18.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.18.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.18.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.18.conv3.weight', 'module.image_feature_extractor.backbone.layer3.18.bn3.weight', 'module.image_feature_extractor.backbone.layer3.18.bn3.bias', 'module.image_feature_extractor.backbone.layer3.18.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.18.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.18.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.19.conv1.weight', 'module.image_feature_extractor.backbone.layer3.19.bn1.weight', 'module.image_feature_extractor.backbone.layer3.19.bn1.bias', 'module.image_feature_extractor.backbone.layer3.19.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.19.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.19.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.19.conv2.weight', 'module.image_feature_extractor.backbone.layer3.19.bn2.weight', 'module.image_feature_extractor.backbone.layer3.19.bn2.bias', 'module.image_feature_extractor.backbone.layer3.19.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.19.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.19.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.19.conv3.weight', 'module.image_feature_extractor.backbone.layer3.19.bn3.weight', 'module.image_feature_extractor.backbone.layer3.19.bn3.bias', 'module.image_feature_extractor.backbone.layer3.19.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.19.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.19.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.20.conv1.weight', 'module.image_feature_extractor.backbone.layer3.20.bn1.weight', 'module.image_feature_extractor.backbone.layer3.20.bn1.bias', 'module.image_feature_extractor.backbone.layer3.20.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.20.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.20.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.20.conv2.weight', 'module.image_feature_extractor.backbone.layer3.20.bn2.weight', 'module.image_feature_extractor.backbone.layer3.20.bn2.bias', 'module.image_feature_extractor.backbone.layer3.20.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.20.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.20.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.20.conv3.weight', 'module.image_feature_extractor.backbone.layer3.20.bn3.weight', 'module.image_feature_extractor.backbone.layer3.20.bn3.bias', 'module.image_feature_extractor.backbone.layer3.20.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.20.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.20.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.21.conv1.weight', 'module.image_feature_extractor.backbone.layer3.21.bn1.weight', 'module.image_feature_extractor.backbone.layer3.21.bn1.bias', 'module.image_feature_extractor.backbone.layer3.21.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.21.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.21.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.21.conv2.weight', 'module.image_feature_extractor.backbone.layer3.21.bn2.weight', 'module.image_feature_extractor.backbone.layer3.21.bn2.bias', 'module.image_feature_extractor.backbone.layer3.21.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.21.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.21.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.21.conv3.weight', 'module.image_feature_extractor.backbone.layer3.21.bn3.weight', 'module.image_feature_extractor.backbone.layer3.21.bn3.bias', 'module.image_feature_extractor.backbone.layer3.21.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.21.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.21.bn3.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.22.conv1.weight', 'module.image_feature_extractor.backbone.layer3.22.bn1.weight', 'module.image_feature_extractor.backbone.layer3.22.bn1.bias', 'module.image_feature_extractor.backbone.layer3.22.bn1.running_mean', 'module.image_feature_extractor.backbone.layer3.22.bn1.running_var', 'module.image_feature_extractor.backbone.layer3.22.bn1.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.22.conv2.weight', 'module.image_feature_extractor.backbone.layer3.22.bn2.weight', 'module.image_feature_extractor.backbone.layer3.22.bn2.bias', 'module.image_feature_extractor.backbone.layer3.22.bn2.running_mean', 'module.image_feature_extractor.backbone.layer3.22.bn2.running_var', 'module.image_feature_extractor.backbone.layer3.22.bn2.num_batches_tracked', 'module.image_feature_extractor.backbone.layer3.22.conv3.weight', 'module.image_feature_extractor.backbone.layer3.22.bn3.weight', 'module.image_feature_extractor.backbone.layer3.22.bn3.bias', 'module.image_feature_extractor.backbone.layer3.22.bn3.running_mean', 'module.image_feature_extractor.backbone.layer3.22.bn3.running_var', 'module.image_feature_extractor.backbone.layer3.22.bn3.num_batches_tracked', 'module.image_feature_extractor.roi_head_feature_extractor.0.conv1.weight', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn1.weight', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn1.bias', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn1.running_mean', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn1.running_var', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn1.num_batches_tracked', 'module.image_feature_extractor.roi_head_feature_extractor.0.conv2.weight', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn2.weight', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn2.bias', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn2.running_mean', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn2.running_var', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn2.num_batches_tracked', 'module.image_feature_extractor.roi_head_feature_extractor.0.conv3.weight', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn3.weight', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn3.bias', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn3.running_mean', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn3.running_var', 'module.image_feature_extractor.roi_head_feature_extractor.0.bn3.num_batches_tracked', 'module.image_feature_extractor.roi_head_feature_extractor.0.downsample.0.weight', 'module.image_feature_extractor.roi_head_feature_extractor.0.downsample.1.weight', 'module.image_feature_extractor.roi_head_feature_extractor.0.downsample.1.bias', 'module.image_feature_extractor.roi_head_feature_extractor.0.downsample.1.running_mean', 'module.image_feature_extractor.roi_head_feature_extractor.0.downsample.1.running_var', 'module.image_feature_extractor.roi_head_feature_extractor.0.downsample.1.num_batches_tracked', 'module.image_feature_extractor.roi_head_feature_extractor.1.conv1.weight', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn1.weight', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn1.bias', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn1.running_mean', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn1.running_var', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn1.num_batches_tracked', 'module.image_feature_extractor.roi_head_feature_extractor.1.conv2.weight', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn2.weight', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn2.bias', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn2.running_mean', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn2.running_var', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn2.num_batches_tracked', 'module.image_feature_extractor.roi_head_feature_extractor.1.conv3.weight', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn3.weight', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn3.bias', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn3.running_mean', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn3.running_var', 'module.image_feature_extractor.roi_head_feature_extractor.1.bn3.num_batches_tracked', 'module.image_feature_extractor.roi_head_feature_extractor.2.conv1.weight', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn1.weight', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn1.bias', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn1.running_mean', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn1.running_var', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn1.num_batches_tracked', 'module.image_feature_extractor.roi_head_feature_extractor.2.conv2.weight', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn2.weight', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn2.bias', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn2.running_mean', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn2.running_var', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn2.num_batches_tracked', 'module.image_feature_extractor.roi_head_feature_extractor.2.conv3.weight', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn3.weight', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn3.bias', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn3.running_mean', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn3.running_var', 'module.image_feature_extractor.roi_head_feature_extractor.2.bn3.num_batches_tracked', 'module.image_feature_extractor.head.0.0.conv1.weight', 'module.image_feature_extractor.head.0.0.bn1.weight', 'module.image_feature_extractor.head.0.0.bn1.bias', 'module.image_feature_extractor.head.0.0.bn1.running_mean', 'module.image_feature_extractor.head.0.0.bn1.running_var', 'module.image_feature_extractor.head.0.0.bn1.num_batches_tracked', 'module.image_feature_extractor.head.0.0.conv2.weight', 'module.image_feature_extractor.head.0.0.bn2.weight', 'module.image_feature_extractor.head.0.0.bn2.bias', 'module.image_feature_extractor.head.0.0.bn2.running_mean', 'module.image_feature_extractor.head.0.0.bn2.running_var', 'module.image_feature_extractor.head.0.0.bn2.num_batches_tracked', 'module.image_feature_extractor.head.0.0.conv3.weight', 'module.image_feature_extractor.head.0.0.bn3.weight', 'module.image_feature_extractor.head.0.0.bn3.bias', 'module.image_feature_extractor.head.0.0.bn3.running_mean', 'module.image_feature_extractor.head.0.0.bn3.running_var', 'module.image_feature_extractor.head.0.0.bn3.num_batches_tracked', 'module.image_feature_extractor.head.0.0.downsample.0.weight', 'module.image_feature_extractor.head.0.0.downsample.1.weight', 'module.image_feature_extractor.head.0.0.downsample.1.bias', 'module.image_feature_extractor.head.0.0.downsample.1.running_mean', 'module.image_feature_extractor.head.0.0.downsample.1.running_var', 'module.image_feature_extractor.head.0.0.downsample.1.num_batches_tracked', 'module.image_feature_extractor.head.0.1.conv1.weight', 'module.image_feature_extractor.head.0.1.bn1.weight', 'module.image_feature_extractor.head.0.1.bn1.bias', 'module.image_feature_extractor.head.0.1.bn1.running_mean', 'module.image_feature_extractor.head.0.1.bn1.running_var', 'module.image_feature_extractor.head.0.1.bn1.num_batches_tracked', 'module.image_feature_extractor.head.0.1.conv2.weight', 'module.image_feature_extractor.head.0.1.bn2.weight', 'module.image_feature_extractor.head.0.1.bn2.bias', 'module.image_feature_extractor.head.0.1.bn2.running_mean', 'module.image_feature_extractor.head.0.1.bn2.running_var', 'module.image_feature_extractor.head.0.1.bn2.num_batches_tracked', 'module.image_feature_extractor.head.0.1.conv3.weight', 'module.image_feature_extractor.head.0.1.bn3.weight', 'module.image_feature_extractor.head.0.1.bn3.bias', 'module.image_feature_extractor.head.0.1.bn3.running_mean', 'module.image_feature_extractor.head.0.1.bn3.running_var', 'module.image_feature_extractor.head.0.1.bn3.num_batches_tracked', 'module.image_feature_extractor.head.0.2.conv1.weight', 'module.image_feature_extractor.head.0.2.bn1.weight', 'module.image_feature_extractor.head.0.2.bn1.bias', 'module.image_feature_extractor.head.0.2.bn1.running_mean', 'module.image_feature_extractor.head.0.2.bn1.running_var', 'module.image_feature_extractor.head.0.2.bn1.num_batches_tracked', 'module.image_feature_extractor.head.0.2.conv2.weight', 'module.image_feature_extractor.head.0.2.bn2.weight', 'module.image_feature_extractor.head.0.2.bn2.bias', 'module.image_feature_extractor.head.0.2.bn2.running_mean', 'module.image_feature_extractor.head.0.2.bn2.running_var', 'module.image_feature_extractor.head.0.2.bn2.num_batches_tracked', 'module.image_feature_extractor.head.0.2.conv3.weight', 'module.image_feature_extractor.head.0.2.bn3.weight', 'module.image_feature_extractor.head.0.2.bn3.bias', 'module.image_feature_extractor.head.0.2.bn3.running_mean', 'module.image_feature_extractor.head.0.2.bn3.running_var', 'module.image_feature_extractor.head.0.2.bn3.num_batches_tracked', 'module.image_feature_extractor.obj_downsample.1.weight', 'module.image_feature_extractor.obj_downsample.1.bias', 'module.object_linguistic_embeddings.weight', 'module.vlbert.word_embeddings.weight', 'module.vlbert.end_embedding.weight', 'module.vlbert.position_embeddings.weight', 'module.vlbert.token_type_embeddings.weight', 'module.vlbert.embedding_LayerNorm.weight', 'module.vlbert.embedding_LayerNorm.bias', 'module.vlbert.visual_ln_text.weight', 'module.vlbert.visual_ln_text.bias', 'module.vlbert.visual_ln_object.weight', 'module.vlbert.visual_ln_object.bias', 'module.vlbert.encoder.layer.0.attention.self.query.weight', 'module.vlbert.encoder.layer.0.attention.self.query.bias', 'module.vlbert.encoder.layer.0.attention.self.key.weight', 'module.vlbert.encoder.layer.0.attention.self.key.bias', 'module.vlbert.encoder.layer.0.attention.self.value.weight', 'module.vlbert.encoder.layer.0.attention.self.value.bias', 'module.vlbert.encoder.layer.0.attention.output.dense.weight', 'module.vlbert.encoder.layer.0.attention.output.dense.bias', 'module.vlbert.encoder.layer.0.attention.output.LayerNorm.weight', 'module.vlbert.encoder.layer.0.attention.output.LayerNorm.bias', 'module.vlbert.encoder.layer.0.intermediate.dense.weight', 'module.vlbert.encoder.layer.0.intermediate.dense.bias', 'module.vlbert.encoder.layer.0.output.dense.weight', 'module.vlbert.encoder.layer.0.output.dense.bias', 'module.vlbert.encoder.layer.0.output.LayerNorm.weight', 'module.vlbert.encoder.layer.0.output.LayerNorm.bias', 'module.vlbert.encoder.layer.1.attention.self.query.weight', 'module.vlbert.encoder.layer.1.attention.self.query.bias', 'module.vlbert.encoder.layer.1.attention.self.key.weight', 'module.vlbert.encoder.layer.1.attention.self.key.bias', 'module.vlbert.encoder.layer.1.attention.self.value.weight', 'module.vlbert.encoder.layer.1.attention.self.value.bias', 'module.vlbert.encoder.layer.1.attention.output.dense.weight', 'module.vlbert.encoder.layer.1.attention.output.dense.bias', 'module.vlbert.encoder.layer.1.attention.output.LayerNorm.weight', 'module.vlbert.encoder.layer.1.attention.output.LayerNorm.bias', 'module.vlbert.encoder.layer.1.intermediate.dense.weight', 'module.vlbert.encoder.layer.1.intermediate.dense.bias', 'module.vlbert.encoder.layer.1.output.dense.weight', 'module.vlbert.encoder.layer.1.output.dense.bias', 'module.vlbert.encoder.layer.1.output.LayerNorm.weight', 'module.vlbert.encoder.layer.1.output.LayerNorm.bias', 'module.vlbert.encoder.layer.2.attention.self.query.weight', 'module.vlbert.encoder.layer.2.attention.self.query.bias', 'module.vlbert.encoder.layer.2.attention.self.key.weight', 'module.vlbert.encoder.layer.2.attention.self.key.bias', 'module.vlbert.encoder.layer.2.attention.self.value.weight', 'module.vlbert.encoder.layer.2.attention.self.value.bias', 'module.vlbert.encoder.layer.2.attention.output.dense.weight', 'module.vlbert.encoder.layer.2.attention.output.dense.bias', 'module.vlbert.encoder.layer.2.attention.output.LayerNorm.weight', 'module.vlbert.encoder.layer.2.attention.output.LayerNorm.bias', 'module.vlbert.encoder.layer.2.intermediate.dense.weight', 'module.vlbert.encoder.layer.2.intermediate.dense.bias', 'module.vlbert.encoder.layer.2.output.dense.weight', 'module.vlbert.encoder.layer.2.output.dense.bias', 'module.vlbert.encoder.layer.2.output.LayerNorm.weight', 'module.vlbert.encoder.layer.2.output.LayerNorm.bias', 'module.vlbert.encoder.layer.3.attention.self.query.weight', 'module.vlbert.encoder.layer.3.attention.self.query.bias', 'module.vlbert.encoder.layer.3.attention.self.key.weight', 'module.vlbert.encoder.layer.3.attention.self.key.bias', 'module.vlbert.encoder.layer.3.attention.self.value.weight', 'module.vlbert.encoder.layer.3.attention.self.value.bias', 'module.vlbert.encoder.layer.3.attention.output.dense.weight', 'module.vlbert.encoder.layer.3.attention.output.dense.bias', 'module.vlbert.encoder.layer.3.attention.output.LayerNorm.weight', 'module.vlbert.encoder.layer.3.attention.output.LayerNorm.bias', 'module.vlbert.encoder.layer.3.intermediate.dense.weight', 'module.vlbert.encoder.layer.3.intermediate.dense.bias', 'module.vlbert.encoder.layer.3.output.dense.weight', 'module.vlbert.encoder.layer.3.output.dense.bias', 'module.vlbert.encoder.layer.3.output.LayerNorm.weight', 'module.vlbert.encoder.layer.3.output.LayerNorm.bias', 'module.vlbert.encoder.layer.4.attention.self.query.weight', 'module.vlbert.encoder.layer.4.attention.self.query.bias', 'module.vlbert.encoder.layer.4.attention.self.key.weight', 'module.vlbert.encoder.layer.4.attention.self.key.bias', 'module.vlbert.encoder.layer.4.attention.self.value.weight', 'module.vlbert.encoder.layer.4.attention.self.value.bias', 'module.vlbert.encoder.layer.4.attention.output.dense.weight', 'module.vlbert.encoder.layer.4.attention.output.dense.bias', 'module.vlbert.encoder.layer.4.attention.output.LayerNorm.weight', 'module.vlbert.encoder.layer.4.attention.output.LayerNorm.bias', 'module.vlbert.encoder.layer.4.intermediate.dense.weight', 'module.vlbert.encoder.layer.4.intermediate.dense.bias', 'module.vlbert.encoder.layer.4.output.dense.weight', 'module.vlbert.encoder.layer.4.output.dense.bias', 'module.vlbert.encoder.layer.4.output.LayerNorm.weight', 'module.vlbert.encoder.layer.4.output.LayerNorm.bias', 'module.vlbert.encoder.layer.5.attention.self.query.weight', 'module.vlbert.encoder.layer.5.attention.self.query.bias', 'module.vlbert.encoder.layer.5.attention.self.key.weight', 'module.vlbert.encoder.layer.5.attention.self.key.bias', 'module.vlbert.encoder.layer.5.attention.self.value.weight', 'module.vlbert.encoder.layer.5.attention.self.value.bias', 'module.vlbert.encoder.layer.5.attention.output.dense.weight', 'module.vlbert.encoder.layer.5.attention.output.dense.bias', 'module.vlbert.encoder.layer.5.attention.output.LayerNorm.weight', 'module.vlbert.encoder.layer.5.attention.output.LayerNorm.bias', 'module.vlbert.encoder.layer.5.intermediate.dense.weight', 'module.vlbert.encoder.layer.5.intermediate.dense.bias', 'module.vlbert.encoder.layer.5.output.dense.weight', 'module.vlbert.encoder.layer.5.output.dense.bias', 'module.vlbert.encoder.layer.5.output.LayerNorm.weight', 'module.vlbert.encoder.layer.5.output.LayerNorm.bias', 'module.vlbert.encoder.layer.6.attention.self.query.weight', 'module.vlbert.encoder.layer.6.attention.self.query.bias', 'module.vlbert.encoder.layer.6.attention.self.key.weight', 'module.vlbert.encoder.layer.6.attention.self.key.bias', 'module.vlbert.encoder.layer.6.attention.self.value.weight', 'module.vlbert.encoder.layer.6.attention.self.value.bias', 'module.vlbert.encoder.layer.6.attention.output.dense.weight', 'module.vlbert.encoder.layer.6.attention.output.dense.bias', 'module.vlbert.encoder.layer.6.attention.output.LayerNorm.weight', 'module.vlbert.encoder.layer.6.attention.output.LayerNorm.bias', 'module.vlbert.encoder.layer.6.intermediate.dense.weight', 'module.vlbert.encoder.layer.6.intermediate.dense.bias', 'module.vlbert.encoder.layer.6.output.dense.weight', 'module.vlbert.encoder.layer.6.output.dense.bias', 'module.vlbert.encoder.layer.6.output.LayerNorm.weight', 'module.vlbert.encoder.layer.6.output.LayerNorm.bias', 'module.vlbert.encoder.layer.7.attention.self.query.weight', 'module.vlbert.encoder.layer.7.attention.self.query.bias', 'module.vlbert.encoder.layer.7.attention.self.key.weight', 'module.vlbert.encoder.layer.7.attention.self.key.bias', 'module.vlbert.encoder.layer.7.attention.self.value.weight', 'module.vlbert.encoder.layer.7.attention.self.value.bias', 'module.vlbert.encoder.layer.7.attention.output.dense.weight', 'module.vlbert.encoder.layer.7.attention.output.dense.bias', 'module.vlbert.encoder.layer.7.attention.output.LayerNorm.weight', 'module.vlbert.encoder.layer.7.attention.output.LayerNorm.bias', 'module.vlbert.encoder.layer.7.intermediate.dense.weight', 'module.vlbert.encoder.layer.7.intermediate.dense.bias', 'module.vlbert.encoder.layer.7.output.dense.weight', 'module.vlbert.encoder.layer.7.output.dense.bias', 'module.vlbert.encoder.layer.7.output.LayerNorm.weight', 'module.vlbert.encoder.layer.7.output.LayerNorm.bias', 'module.vlbert.encoder.layer.8.attention.self.query.weight', 'module.vlbert.encoder.layer.8.attention.self.query.bias', 'module.vlbert.encoder.layer.8.attention.self.key.weight', 'module.vlbert.encoder.layer.8.attention.self.key.bias', 'module.vlbert.encoder.layer.8.attention.self.value.weight', 'module.vlbert.encoder.layer.8.attention.self.value.bias', 'module.vlbert.encoder.layer.8.attention.output.dense.weight', 'module.vlbert.encoder.layer.8.attention.output.dense.bias', 'module.vlbert.encoder.layer.8.attention.output.LayerNorm.weight', 'module.vlbert.encoder.layer.8.attention.output.LayerNorm.bias', 'module.vlbert.encoder.layer.8.intermediate.dense.weight', 'module.vlbert.encoder.layer.8.intermediate.dense.bias', 'module.vlbert.encoder.layer.8.output.dense.weight', 'module.vlbert.encoder.layer.8.output.dense.bias', 'module.vlbert.encoder.layer.8.output.LayerNorm.weight', 'module.vlbert.encoder.layer.8.output.LayerNorm.bias', 'module.vlbert.encoder.layer.9.attention.self.query.weight', 'module.vlbert.encoder.layer.9.attention.self.query.bias', 'module.vlbert.encoder.layer.9.attention.self.key.weight', 'module.vlbert.encoder.layer.9.attention.self.key.bias', 'module.vlbert.encoder.layer.9.attention.self.value.weight', 'module.vlbert.encoder.layer.9.attention.self.value.bias', 'module.vlbert.encoder.layer.9.attention.output.dense.weight', 'module.vlbert.encoder.layer.9.attention.output.dense.bias', 'module.vlbert.encoder.layer.9.attention.output.LayerNorm.weight', 'module.vlbert.encoder.layer.9.attention.output.LayerNorm.bias', 'module.vlbert.encoder.layer.9.intermediate.dense.weight', 'module.vlbert.encoder.layer.9.intermediate.dense.bias', 'module.vlbert.encoder.layer.9.output.dense.weight', 'module.vlbert.encoder.layer.9.output.dense.bias', 'module.vlbert.encoder.layer.9.output.LayerNorm.weight', 'module.vlbert.encoder.layer.9.output.LayerNorm.bias', 'module.vlbert.encoder.layer.10.attention.self.query.weight', 'module.vlbert.encoder.layer.10.attention.self.query.bias', 'module.vlbert.encoder.layer.10.attention.self.key.weight', 'module.vlbert.encoder.layer.10.attention.self.key.bias', 'module.vlbert.encoder.layer.10.attention.self.value.weight', 'module.vlbert.encoder.layer.10.attention.self.value.bias', 'module.vlbert.encoder.layer.10.attention.output.dense.weight', 'module.vlbert.encoder.layer.10.attention.output.dense.bias', 'module.vlbert.encoder.layer.10.attention.output.LayerNorm.weight', 'module.vlbert.encoder.layer.10.attention.output.LayerNorm.bias', 'module.vlbert.encoder.layer.10.intermediate.dense.weight', 'module.vlbert.encoder.layer.10.intermediate.dense.bias', 'module.vlbert.encoder.layer.10.output.dense.weight', 'module.vlbert.encoder.layer.10.output.dense.bias', 'module.vlbert.encoder.layer.10.output.LayerNorm.weight', 'module.vlbert.encoder.layer.10.output.LayerNorm.bias', 'module.vlbert.encoder.layer.11.attention.self.query.weight', 'module.vlbert.encoder.layer.11.attention.self.query.bias', 'module.vlbert.encoder.layer.11.attention.self.key.weight', 'module.vlbert.encoder.layer.11.attention.self.key.bias', 'module.vlbert.encoder.layer.11.attention.self.value.weight', 'module.vlbert.encoder.layer.11.attention.self.value.bias', 'module.vlbert.encoder.layer.11.attention.output.dense.weight', 'module.vlbert.encoder.layer.11.attention.output.dense.bias', 'module.vlbert.encoder.layer.11.attention.output.LayerNorm.weight', 'module.vlbert.encoder.layer.11.attention.output.LayerNorm.bias', 'module.vlbert.encoder.layer.11.intermediate.dense.weight', 'module.vlbert.encoder.layer.11.intermediate.dense.bias', 'module.vlbert.encoder.layer.11.output.dense.weight', 'module.vlbert.encoder.layer.11.output.dense.bias', 'module.vlbert.encoder.layer.11.output.LayerNorm.weight', 'module.vlbert.encoder.layer.11.output.LayerNorm.bias', 'module.final_mlp.0.dense.weight', 'module.final_mlp.0.dense.bias'])
[Partial Load] non matched keys: ['object_mask_word_embedding.weight', 'aux_text_visual_embedding.weight', 'vlbert.mlm_head.predictions.bias', 'vlbert.mlm_head.predictions.transform.dense.weight', 'vlbert.mlm_head.predictions.transform.dense.bias', 'vlbert.mlm_head.predictions.transform.LayerNorm.weight', 'vlbert.mlm_head.predictions.transform.LayerNorm.bias', 'vlbert.mlm_head.predictions.decoder.weight', 'vlbert.mvrc_head.region_cls_pred.weight', 'vlbert.mvrc_head.region_cls_pred.bias']
[Partial Load] non pretrain keys: ['module.final_mlp.2.weight', 'module.final_mlp.2.bias']
PROGRESS: 0.00%
Traceback (most recent call last):
  File "refcoco/train_end2end.py", line 61, in <module>
    main()
  File "refcoco/train_end2end.py", line 55, in main
    rank, model = train_net(args, config)
  File "/fs/classhomes/fall2020/cmsc828w/c828w038/Visual-grounding/VLBERT/VL-BERT/refcoco/../refcoco/function/train.py", line 323, in train_net
    gradient_accumulate_steps=config.TRAIN.GRAD_ACCUMULATE_STEPS)
  File "/fs/classhomes/fall2020/cmsc828w/c828w038/Visual-grounding/VLBERT/VL-BERT/refcoco/../common/trainer.py", line 101, in train
    for nbatch, batch in enumerate(train_loader):
  File "/fs/classhomes/fall2020/cmsc828w/c828w038/miniconda3/envs/vl-bert/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 582, in __next__
    return self._process_next_batch(batch)
  File "/fs/classhomes/fall2020/cmsc828w/c828w038/miniconda3/envs/vl-bert/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 608, in _process_next_batch
    raise batch.exc_type(batch.exc_msg)
FileNotFoundError: Traceback (most recent call last):
  File "/fs/classhomes/fall2020/cmsc828w/c828w038/miniconda3/envs/vl-bert/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py", line 99, in _worker_loop
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/fs/classhomes/fall2020/cmsc828w/c828w038/miniconda3/envs/vl-bert/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py", line 99, in <listcomp>
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/fs/classhomes/fall2020/cmsc828w/c828w038/Visual-grounding/VLBERT/VL-BERT/refcoco/../refcoco/data/datasets/refcoco.py", line 128, in __getitem__
    image = self._load_image(idb['image_fn'])
  File "/fs/classhomes/fall2020/cmsc828w/c828w038/Visual-grounding/VLBERT/VL-BERT/refcoco/../refcoco/data/datasets/refcoco.py", line 331, in _load_image
    return Image.open(path).convert('RGB')
  File "/fs/classhomes/fall2020/cmsc828w/c828w038/miniconda3/envs/vl-bert/lib/python3.6/site-packages/PIL/Image.py", line 2891, in open
    fp = builtins.open(filename, "rb")
FileNotFoundError: [Errno 2] No such file or directory: './data/coco/train2014/COCO_train2014_000000243907.jpg'

Traceback (most recent call last):
  File "./scripts/launch.py", line 202, in <module>
    main()
  File "./scripts/launch.py", line 198, in main
    cmd=process.args)
subprocess.CalledProcessError: Command '['/fs/classhomes/fall2020/cmsc828w/c828w038/miniconda3/envs/vl-bert/bin/python', '-u', 'refcoco/train_end2end.py', '--cfg', 'cfgs/refcoco/base_gt_boxes_4x16G.yaml', '--model-dir', 'checkpoints/', '--dist']' returned non-zero exit status 1.

